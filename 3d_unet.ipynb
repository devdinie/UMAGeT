{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a 3D U-Net\n",
    "\n",
    "TensorFlow 2 code to train a 3D U-Net on the brain tumor segmentation ([BraTS](https://www.med.upenn.edu/sbia/brats2017.html)) subset of the [Medical Segmentation Decathlon dataset](http://medicaldecathlon.com/) dataset. \n",
    "\n",
    "This model can achieve a [Dice coefficient](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1415224/) of > 0.80 on the whole tumor using just the [FLAIR](https://en.wikipedia.org/wiki/Fluid-attenuated_inversion_recovery) channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras as K\n",
    "\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine if we are using Intel-optimized TensorFlow (DNNL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_intel_tensorflow():\n",
    "    \"\"\"\n",
    "    Check if Intel version of TensorFlow is installed\n",
    "    \"\"\"\n",
    "    import tensorflow as tf\n",
    "\n",
    "    print(\"We are using Tensorflow version {}\".format(tf.__version__))\n",
    "\n",
    "    major_version = int(tf.__version__.split(\".\")[0])\n",
    "    if major_version >= 2:\n",
    "        from tensorflow.python import _pywrap_util_port\n",
    "        print(\"Intel-optimizations (DNNL) enabled:\",\n",
    "              _pywrap_util_port.IsMklEnabled())\n",
    "    else:\n",
    "        print(\"Intel-optimizations (DNNL) enabled:\",\n",
    "              tf.pywrap_tensorflow.IsMklEnabled())\n",
    "\n",
    "\n",
    "test_intel_tensorflow()  # Prints if Intel-optimized TensorFlow is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path=\"/data/medical_decathlon/Task01_BrainTumour/\"\n",
    "train_val_split = 0.80\n",
    "val_test_split = 0.50\n",
    "bz_train=8\n",
    "bz_val=4\n",
    "bz_test=1\n",
    "num_epochs=30\n",
    "\n",
    "crop_dim = (128,128,128,1)\n",
    "number_output_classes = 3\n",
    "\n",
    "filters=8\n",
    "saved_model_name = \"3d_unet_decathlon\"\n",
    "\n",
    "seed=816\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a data loader\n",
    "\n",
    "We'll use `tf.data` to define a way to load the BraTS dataset at runtime whenever a new batch of 3D images and masks are requested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetGenerator:\n",
    "        \n",
    "    def __init__(self, data_path):\n",
    "        \n",
    "        self.data_path = data_path\n",
    "        self.create_file_list()\n",
    "\n",
    "    def create_file_list(self):\n",
    "        \"\"\"\n",
    "        Get list of the files from the BraTS raw data\n",
    "        Split into training and testing sets.\n",
    "        \"\"\"\n",
    "        import os\n",
    "        import json\n",
    "        \n",
    "        json_filename = os.path.join(self.data_path, \"dataset.json\")\n",
    "\n",
    "        try:\n",
    "            with open(json_filename, \"r\") as fp:\n",
    "                experiment_data = json.load(fp)\n",
    "        except IOError as e:\n",
    "            print(\"File {} doesn't exist. It should be part of the \"\n",
    "                  \"Decathlon directory\".format(json_filename))\n",
    "\n",
    "        self.output_channels = experiment_data[\"labels\"]\n",
    "        self.input_channels = experiment_data[\"modality\"]\n",
    "        self.description = experiment_data[\"description\"]\n",
    "        self.name = experiment_data[\"name\"]\n",
    "        self.release = experiment_data[\"release\"]\n",
    "        self.license = experiment_data[\"licence\"]\n",
    "        self.reference = experiment_data[\"reference\"]\n",
    "        self.tensorImageSize = experiment_data[\"tensorImageSize\"]\n",
    "        self.numFiles = experiment_data[\"numTraining\"]\n",
    "        \n",
    "        \"\"\"\n",
    "        Create a dictionary of tuples with image filename and label filename\n",
    "        \"\"\"\n",
    "        self.filenames = {}\n",
    "        for idx in range(self.numFiles):\n",
    "            self.filenames[idx] = [os.path.join(self.data_path,\n",
    "                                              experiment_data[\"training\"][idx][\"image\"]),\n",
    "                                    os.path.join(self.data_path,\n",
    "                                              experiment_data[\"training\"][idx][\"label\"])]\n",
    "            \n",
    "        \n",
    "    def print_info(self):\n",
    "        \"\"\"\n",
    "        Print the dataset information\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"=\"*30)\n",
    "        print(\"Dataset name:        \", self.name)\n",
    "        print(\"Dataset description: \", self.description)\n",
    "        print(\"Tensor image size:   \", self.tensorImageSize)\n",
    "        print(\"Dataset release:     \", self.release)\n",
    "        print(\"Dataset reference:   \", self.reference)\n",
    "        print(\"Input channels:      \", self.input_channels)\n",
    "        print(\"Output labels:       \", self.output_channels)\n",
    "        print(\"Dataset license:     \", self.license)\n",
    "        print(\"=\"*30)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "brats_datafiles = DatasetGenerator(data_path)\n",
    "brats_datafiles.print_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "\n",
    "Here we preprocess the 3D MRI scans. We'll normalize the images, crop the images, and do random flips/rotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_normalize_img(img):\n",
    "    \"\"\"\n",
    "    Normalize the image so that the mean value for each image\n",
    "    is 0 and the standard deviation is 1.\n",
    "    \"\"\"\n",
    "    for channel in range(img.shape[-1]):\n",
    "\n",
    "        img_temp = img[..., channel]\n",
    "        img_temp = (img_temp - np.mean(img_temp)) / np.std(img_temp)\n",
    "\n",
    "        img[..., channel] = img_temp\n",
    "\n",
    "    return img\n",
    "    \n",
    "def crop(img, msk, randomize):\n",
    "        \"\"\"\n",
    "        Randomly crop the image and mask\n",
    "        \"\"\"\n",
    "\n",
    "        slices = []\n",
    "        \n",
    "        # Do we randomize?\n",
    "        is_random = randomize and np.random.rand() > 0.5\n",
    "\n",
    "        for idx in range(len(img.shape)-1):  # Go through each dimension\n",
    "\n",
    "            cropLen = crop_dim[idx]\n",
    "            imgLen = img.shape[idx]\n",
    "\n",
    "            start = (imgLen-cropLen)//2\n",
    "\n",
    "            ratio_crop = 0.20  # Crop up this this % of pixels for offset\n",
    "            # Number of pixels to offset crop in this dimension\n",
    "            offset = int(np.floor(start*ratio_crop))\n",
    "\n",
    "            if offset > 0:\n",
    "                if is_random:\n",
    "                    start += np.random.choice(range(-offset, offset))\n",
    "                    if ((start + cropLen) > imgLen):  # Don't fall off the image\n",
    "                        start = (imgLen-cropLen)//2\n",
    "            else:\n",
    "                start = 0\n",
    "\n",
    "            slices.append(slice(start, start+cropLen))\n",
    "\n",
    "        return img[tuple(slices)], msk[tuple(slices)]\n",
    "    \n",
    "def augment_data(img, msk, crop_dim):\n",
    "    \"\"\"\n",
    "    Data augmentation\n",
    "    Flip image and mask. Rotate image and mask.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Determine if axes are equal and can be rotated\n",
    "    # If the axes aren't equal then we can't rotate them.\n",
    "    equal_dim_axis = []\n",
    "    for idx in range(0, len(crop_dim)):\n",
    "        for jdx in range(idx+1, len(crop_dim)):\n",
    "            if crop_dim[idx] == crop_dim[jdx]:\n",
    "                equal_dim_axis.append([idx, jdx])  # Valid rotation axes\n",
    "    dim_to_rotate = equal_dim_axis\n",
    "\n",
    "    if np.random.rand() > 0.5:\n",
    "        # Random 0,1 (axes to flip)\n",
    "        ax = np.random.choice(np.arange(len(crop_dim)-1))\n",
    "        img = np.flip(img, ax)\n",
    "        msk = np.flip(msk, ax)\n",
    "\n",
    "    elif (len(dim_to_rotate) > 0) and (np.random.rand() > 0.5):\n",
    "        rot = np.random.choice([1, 2, 3])  # 90, 180, or 270 degrees\n",
    "\n",
    "        # This will choose the axes to rotate\n",
    "        # Axes must be equal in size\n",
    "        random_axis = dim_to_rotate[np.random.choice(len(dim_to_rotate))]\n",
    "        \n",
    "        img = np.rot90(img, rot, axes=random_axis)  # Rotate axes 0 and 1\n",
    "        msk = np.rot90(msk, rot, axes=random_axis)  # Rotate axes 0 and 1\n",
    "\n",
    "    return img, msk\n",
    "    \n",
    "def read_nifti_file(idx, crop_dim, randomize=False):\n",
    "    \"\"\"\n",
    "    Read Nifti file\n",
    "    \"\"\"\n",
    "    \n",
    "    idx = idx.numpy()\n",
    "    imgFile = brats_datafiles.filenames[idx][0]\n",
    "    mskFile = brats_datafiles.filenames[idx][1]\n",
    "    \n",
    "    img = np.array(nib.load(imgFile).dataobj)\n",
    "    \n",
    "    img = np.rot90(img[...,[0]]) # Just take the FLAIR channel (0)\n",
    "    \n",
    "    msk = np.rot90(np.array(nib.load(mskFile).dataobj))\n",
    "\n",
    "    \"\"\"\n",
    "    \"labels\": {\n",
    "         \"0\": \"background\",\n",
    "         \"1\": \"edema\",\n",
    "         \"2\": \"non-enhancing tumor\",\n",
    "         \"3\": \"enhancing tumour\"}\n",
    "     \"\"\"\n",
    "    # Combine all masks but background\n",
    "    if number_output_classes == 1:\n",
    "        msk[msk > 0] = 1.0\n",
    "        msk = np.expand_dims(msk, -1)\n",
    "    else:\n",
    "        msk_temp = np.zeros(list(msk.shape) + [number_output_classes])\n",
    "        for channel in range(number_output_classes):\n",
    "            msk_temp[msk==channel,channel] = 1.0\n",
    "        msk = msk_temp\n",
    "    \n",
    "    imgFilename = (os.path.basename(brats_datafiles.filenames[idx][0])).split(\".nii.gz\")[0]\n",
    "    \n",
    "    # Crop\n",
    "    img, msk = crop(img, msk, randomize)\n",
    "    \n",
    "    # Normalize\n",
    "    img = z_normalize_img(img)\n",
    "    \n",
    "    # Randomly rotate\n",
    "    if randomize:\n",
    "        img, msk = augment_data(img, msk, crop_dim)\n",
    "    \n",
    "    return img, msk\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.data\n",
    "\n",
    "Define the training, testing, and validation data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numFiles = brats_datafiles.numFiles\n",
    "numTrain = int(numFiles * train_val_split)\n",
    "numValTest = numFiles - numTrain\n",
    "\n",
    "ds = tf.data.Dataset.range(numFiles).shuffle(numFiles, seed) # Shuffle the dataset\n",
    "\n",
    "ds_train = ds.take(numTrain)\n",
    "ds_val_test = ds.skip(numTrain)\n",
    "ds_val = ds_val_test.take(int(numValTest * val_test_split))\n",
    "ds_test = ds_val_test.skip(int(numValTest * val_test_split))\n",
    "\n",
    "ds_train = ds_train.map(lambda x: tf.py_function(read_nifti_file, [x, crop_dim, True], [tf.float32, tf.float32]), num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "ds_val = ds_val.map(lambda x: tf.py_function(read_nifti_file, [x, crop_dim, False], [tf.float32, tf.float32]), num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "ds_test = ds_test.map(lambda x: tf.py_function(read_nifti_file, [x, crop_dim, False], [tf.float32, tf.float32]), num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "ds_train = ds_train.batch(bz_train)\n",
    "ds_train = ds_train.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "ds_val = ds_val.batch(bz_val)\n",
    "ds_val = ds_val.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "ds_test = ds_test.batch(bz_test)\n",
    "ds_test = ds_test.prefetch(tf.data.experimental.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot some data samples\n",
    "\n",
    "Plots the MRI and Tumor Masks from a few data samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "\n",
    "num_cols=2\n",
    "slice_num = 91\n",
    "\n",
    "msk_channel=1\n",
    "img_channel=0 \n",
    "\n",
    "for img, msk in ds_train.take(1):\n",
    "    bs = img.shape[0]\n",
    "    \n",
    "    for idx in range(bs):\n",
    "          \n",
    "        plt.subplot(bs,num_cols,idx*num_cols + 1)\n",
    "        plt.imshow(img[idx,:,:,slice_num,img_channel], cmap=\"bone\")\n",
    "        plt.title(\"MRI {}\".format(brats_datafiles.input_channels[str(img_channel)]), fontsize=18)\n",
    "        plt.subplot(bs,num_cols,idx*num_cols + 2)\n",
    "        plt.imshow(msk[idx,:,:,slice_num,msk_channel], cmap=\"bone\")\n",
    "        plt.title(\"Tumor {}\".format(brats_datafiles.output_channels[str(msk_channel)]), fontsize=18)\n",
    "        \n",
    "print(\"Mean pixel value = {}\".format(np.mean(img[0,:,:,:,0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "\n",
    "num_cols=2\n",
    "slice_num = 91\n",
    "\n",
    "msk_channel=1\n",
    "img_channel=0 \n",
    "\n",
    "for img, msk in ds_val.take(1):\n",
    "    bs = img.shape[0]\n",
    "    \n",
    "    for idx in range(bs):\n",
    "        \n",
    "        plt.subplot(bs,num_cols,idx*num_cols + 1)\n",
    "        plt.imshow(img[idx,:,:,slice_num,img_channel], cmap=\"bone\")\n",
    "        plt.title(\"MRI {}\".format(brats_datafiles.input_channels[str(img_channel)]), fontsize=18)\n",
    "        plt.subplot(bs,num_cols,idx*num_cols + 2)\n",
    "        plt.imshow(msk[idx,:,:,slice_num,msk_channel], cmap=\"bone\")\n",
    "        plt.title(\"Tumor {}\".format(brats_datafiles.output_channels[str(msk_channel)]), fontsize=18)\n",
    "   \n",
    "print(\"Mean pixel value = {}\".format(np.mean(img[0,:,:,:,0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "\n",
    "num_cols=2\n",
    "slice_num = 91\n",
    "\n",
    "msk_channel=1\n",
    "img_channel=0 \n",
    "\n",
    "for img, msk in ds_test.take(1):\n",
    "    bs = img.shape[0]\n",
    "    \n",
    "    for idx in range(bs):\n",
    "          \n",
    "        plt.subplot(bs,num_cols,idx*num_cols + 1)\n",
    "        plt.imshow(img[idx,:,:,slice_num,img_channel], cmap=\"bone\")\n",
    "        plt.title(\"MRI {}\".format(brats_datafiles.input_channels[str(img_channel)]), fontsize=18)\n",
    "        plt.subplot(bs,num_cols,idx*num_cols + 2)\n",
    "        plt.imshow(msk[idx,:,:,slice_num,msk_channel], cmap=\"bone\")\n",
    "        plt.title(\"Tumor {}\".format(brats_datafiles.output_channels[str(msk_channel)]), fontsize=18)\n",
    "   \n",
    "        \n",
    "print(\"Mean pixel value = {}\".format(np.mean(img[0,:,:,:,0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the loss and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef(target, prediction, axis=(1, 2, 3), smooth=0.0001):\n",
    "    \"\"\"\n",
    "    Sorenson Dice\n",
    "    \\frac{  2 \\times \\left | T \\right | \\cap \\left | P \\right |}{ \\left | T \\right | +  \\left | P \\right |  }\n",
    "    where T is ground truth mask and P is the prediction mask\n",
    "    \"\"\"\n",
    "    prediction = tf.round(prediction)  # Round to 0 or 1\n",
    "\n",
    "    intersection = tf.reduce_sum(target * prediction, axis=axis)\n",
    "    union = tf.reduce_sum(target + prediction, axis=axis)\n",
    "    numerator = tf.constant(2.) * intersection + smooth\n",
    "    denominator = union + smooth\n",
    "    coef = numerator / denominator\n",
    "\n",
    "    return tf.reduce_mean(coef)\n",
    "\n",
    "def soft_dice_coef(target, prediction, axis=(1, 2, 3), smooth=0.0001):\n",
    "    \"\"\"\n",
    "    Sorenson (Soft) Dice - Don't round predictions\n",
    "    \\frac{  2 \\times \\left | T \\right | \\cap \\left | P \\right |}{ \\left | T \\right | +  \\left | P \\right |  }\n",
    "    where T is ground truth mask and P is the prediction mask\n",
    "    \"\"\"\n",
    "    intersection = tf.reduce_sum(target * prediction, axis=axis)\n",
    "    union = tf.reduce_sum(target + prediction, axis=axis)\n",
    "    numerator = tf.constant(2.) * intersection + smooth\n",
    "    denominator = union + smooth\n",
    "    coef = numerator / denominator\n",
    "\n",
    "    return tf.reduce_mean(coef)\n",
    "\n",
    "\n",
    "def dice_loss(target, prediction, axis=(1, 2, 3), smooth=0.0001):\n",
    "    \"\"\"\n",
    "    Sorenson (Soft) Dice loss\n",
    "    Using -log(Dice) as the loss since it is better behaved.\n",
    "    Also, the log allows avoidance of the division which\n",
    "    can help prevent underflow when the numbers are very small.\n",
    "    \"\"\"\n",
    "    intersection = tf.reduce_sum(prediction * target, axis=axis)\n",
    "    p = tf.reduce_sum(prediction, axis=axis)\n",
    "    t = tf.reduce_sum(target, axis=axis)\n",
    "    numerator = tf.reduce_mean(intersection + smooth)\n",
    "    denominator = tf.reduce_mean(t + p + smooth)\n",
    "    dice_loss = -tf.math.log(2.*numerator) + tf.math.log(denominator)\n",
    "\n",
    "    return dice_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the 3D U-Net\n",
    "\n",
    "Create a TensorFlow model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet_3d(fms=32, input_dim=crop_dim, use_upsampling=False, concat_axis=-1):\n",
    "    \"\"\"\n",
    "    3D U-Net\n",
    "    \"\"\"\n",
    "    \n",
    "    def ConvolutionBlock(x, name, fms, params):\n",
    "        \"\"\"\n",
    "        Convolutional block of layers\n",
    "        Per the original paper this is back to back 3D convs\n",
    "        with batch norm and then ReLU.\n",
    "        \"\"\"\n",
    "\n",
    "        x = K.layers.Conv3D(filters=fms, **params, name=name+\"_conv0\")(x)\n",
    "        x = K.layers.BatchNormalization(name=name+\"_bn0\")(x)\n",
    "        x = K.layers.Activation(\"relu\", name=name+\"_relu0\")(x)\n",
    "\n",
    "        x = K.layers.Conv3D(filters=fms, **params, name=name+\"_conv1\")(x)\n",
    "        x = K.layers.BatchNormalization(name=name+\"_bn1\")(x)\n",
    "        x = K.layers.Activation(\"relu\", name=name)(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    inputs = K.layers.Input(shape=input_dim, name=\"MRImages\")\n",
    "\n",
    "    params = dict(kernel_size=(3, 3, 3), activation=None,\n",
    "                  padding=\"same\", \n",
    "                  kernel_initializer=\"he_uniform\")\n",
    "\n",
    "    # Transposed convolution parameters\n",
    "    params_trans = dict(kernel_size=(2, 2, 2), strides=(2, 2, 2),\n",
    "                        padding=\"same\")\n",
    "\n",
    "\n",
    "    # BEGIN - Encoding path\n",
    "    encodeA = ConvolutionBlock(inputs, \"encodeA\", fms, params)\n",
    "    poolA = K.layers.MaxPooling3D(name=\"poolA\", pool_size=(2, 2, 2))(encodeA)\n",
    "\n",
    "    encodeB = ConvolutionBlock(poolA, \"encodeB\", fms*2, params)\n",
    "    poolB = K.layers.MaxPooling3D(name=\"poolB\", pool_size=(2, 2, 2))(encodeB)\n",
    "\n",
    "    encodeC = ConvolutionBlock(poolB, \"encodeC\", fms*4, params)\n",
    "    poolC = K.layers.MaxPooling3D(name=\"poolC\", pool_size=(2, 2, 2))(encodeC)\n",
    "\n",
    "    encodeD = ConvolutionBlock(poolC, \"encodeD\", fms*8, params)\n",
    "    poolD = K.layers.MaxPooling3D(name=\"poolD\", pool_size=(2, 2, 2))(encodeD)\n",
    "\n",
    "    encodeE = ConvolutionBlock(poolD, \"encodeE\", fms*16, params)\n",
    "    # END - Encoding path\n",
    "\n",
    "    # BEGIN - Decoding path\n",
    "    if use_upsampling:\n",
    "        up = K.layers.UpSampling3D(name=\"upE\", size=(2, 2, 2),\n",
    "                                   interpolation=\"bilinear\")(encodeE)\n",
    "    else:\n",
    "        up = K.layers.Conv3DTranspose(name=\"transconvE\", filters=fms*8,\n",
    "                                      **params_trans)(encodeE)\n",
    "    concatD = K.layers.concatenate(\n",
    "        [up, encodeD], axis=concat_axis, name=\"concatD\")\n",
    "\n",
    "    decodeC = ConvolutionBlock(concatD, \"decodeC\", fms*8, params)\n",
    "\n",
    "    if use_upsampling:\n",
    "        up = K.layers.UpSampling3D(name=\"upC\", size=(2, 2, 2),\n",
    "                                   interpolation=\"bilinear\")(decodeC)\n",
    "    else:\n",
    "        up = K.layers.Conv3DTranspose(name=\"transconvC\", filters=fms*4,\n",
    "                                      **params_trans)(decodeC)\n",
    "    concatC = K.layers.concatenate(\n",
    "        [up, encodeC], axis=concat_axis, name=\"concatC\")\n",
    "\n",
    "    decodeB = ConvolutionBlock(concatC, \"decodeB\", fms*4, params)\n",
    "\n",
    "    if use_upsampling:\n",
    "        up = K.layers.UpSampling3D(name=\"upB\", size=(2, 2, 2),\n",
    "                                   interpolation=\"bilinear\")(decodeB)\n",
    "    else:\n",
    "        up = K.layers.Conv3DTranspose(name=\"transconvB\", filters=fms*2,\n",
    "                                      **params_trans)(decodeB)\n",
    "    concatB = K.layers.concatenate(\n",
    "        [up, encodeB], axis=concat_axis, name=\"concatB\")\n",
    "\n",
    "    decodeA = ConvolutionBlock(concatB, \"decodeA\", fms*2, params)\n",
    "\n",
    "    if use_upsampling:\n",
    "        up = K.layers.UpSampling3D(name=\"upA\", size=(2, 2, 2),\n",
    "                                   interpolation=\"bilinear\")(decodeA)\n",
    "    else:\n",
    "        up = K.layers.Conv3DTranspose(name=\"transconvA\", filters=fms,\n",
    "                                      **params_trans)(decodeA)\n",
    "    concatA = K.layers.concatenate(\n",
    "        [up, encodeA], axis=concat_axis, name=\"concatA\")\n",
    "\n",
    "    # END - Decoding path\n",
    "\n",
    "    convOut = ConvolutionBlock(concatA, \"convOut\", fms, params)\n",
    "\n",
    "    prediction = K.layers.Conv3D(name=\"PredictionMask\",\n",
    "                                 filters=number_output_classes, kernel_size=(1, 1, 1),\n",
    "                                 activation=\"sigmoid\")(convOut)\n",
    "\n",
    "    model = K.models.Model(inputs=[inputs], outputs=[prediction], name=\"3d_unet_decathlon\")\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = unet_3d(fms=filters, input_dim=crop_dim)\n",
    "\n",
    "model.compile(loss=dice_loss, metrics=[dice_coef, soft_dice_coef], optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the training callbacks\n",
    "\n",
    "This includes model checkpoints and TensorBoard logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = K.callbacks.ModelCheckpoint(saved_model_name,\n",
    "                                         verbose=1,\n",
    "                                         save_best_only=True)\n",
    "\n",
    "# TensorBoard\n",
    "logs_dir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tb_logs = K.callbacks.TensorBoard(log_dir=logs_dir)\n",
    "\n",
    "callbacks = [checkpoint, tb_logs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.fit(ds_train, epochs=num_epochs, validation_data=ds_val, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model\n",
    "\n",
    "Evaluate the final model on the test dataset. This gives us an idea of how the model should perform on data it has never seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, dice_coef, soft_dice_coef = model.evaluate(ds_test)\n",
    "\n",
    "print(\"Average Dice Coefficient on test dataset = {:.4f}\".format(dice_coef))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. SPDX-License-Identifier: EPL-2.0*\n",
    "\n",
    "*Copyright (c) 2019-2020 Intel Corporation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
