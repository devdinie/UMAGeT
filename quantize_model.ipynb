{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantize OpenVINO&trade; Model from FP32 to INT8\n",
    "\n",
    "In this notebook, we'll demonstrate how to convert an OpenVINO&trade; model from FP32 to INT8 precision using the [post-training optimization toolkit API](https://docs.openvinotoolkit.org/latest/pot_compression_api_README.html). \n",
    "\n",
    "We'll assume that you already ran `train.py` and have trained a TensorFlow 3D U-Net model on the BraTS Medical Decathlon dataset.  We'll further assume that you have converted the final TensorFlow 3D U-Net model to OpenVINO&trade; by running something like:\n",
    "\n",
    "```\n",
    "source /opt/intel/openvino/bin/setupvars.sh\n",
    "python $INTEL_OPENVINO_DIR/deployment_tools/model_optimizer/mo_tf.py \\\n",
    "       --saved_model_dir 3d_unet_decathlon_final \\\n",
    "       --model_name 3d_unet_decathlon \\\n",
    "       --batch 1  \\\n",
    "       --output_dir openvino_models/FP32 \\\n",
    "       --data_type FP32\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the OpenVINO&trade; Python API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openvino.inference_engine import IECore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import some other Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset\n",
    "\n",
    "Note: We'll reuse the same data loader we used in training. Nevertheless, all we need to do is to provide the 3D MRI scan with the same preprocessing (normalization, cropping, etc) as a NumPy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import DatasetGenerator\n",
    "import settings\n",
    "\n",
    "crop_dim = (settings.TILE_HEIGHT, settings.TILE_WIDTH,\n",
    "            settings.TILE_DEPTH, settings.NUMBER_INPUT_CHANNELS)\n",
    "\n",
    "settings.BATCH_SIZE = 1 \n",
    "\n",
    "brats_data = DatasetGenerator(crop_dim=crop_dim,\n",
    "                              data_path=settings.DATA_PATH,\n",
    "                              batch_size=settings.BATCH_SIZE,\n",
    "                              train_test_split=settings.TRAIN_TEST_SPLIT,\n",
    "                              validate_test_split=settings.VALIDATE_TEST_SPLIT,\n",
    "                              number_output_classes=settings.NUMBER_OUTPUT_CLASSES,\n",
    "                              random_seed=settings.RANDOM_SEED)\n",
    "\n",
    "brats_data.print_info()  # Print dataset information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenVINO&trade; Post-training Optimization Toolkit Imports\n",
    "\n",
    "This are the additional libraries we need for the OpenVINO&trade; Post-training optimization toolkit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from addict import Dict\n",
    "\n",
    "from compression.api import Metric, DataLoader\n",
    "from compression.engines.ie_engine import IEEngine\n",
    "from compression.graph import load_model, save_model\n",
    "from compression.graph.model_utils import compress_model_weights\n",
    "from compression.pipeline.initializer import create_pipeline\n",
    "from compression.utils.logger import init_logger\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretty print\n",
    "\n",
    "Let's just define some pretty colors to print text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bcolors:\n",
    "    \"\"\"\n",
    "    Just gives us some colors for the text\n",
    "    \"\"\"\n",
    "    HEADER = '\\033[95m'\n",
    "    OKBLUE = '\\033[94m'\n",
    "    OKGREEN = '\\033[92m'\n",
    "    WARNING = '\\033[93m'\n",
    "    FAIL = '\\033[91m'\n",
    "    ENDC = '\\033[0m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-training Optimization Toolkit API Settings\n",
    "\n",
    "Here we define some of the settings for the calibration tool. Most of this just defines where to find the OpenVINO&trade; IR model files (`.xml`,`.bin`). It also defines the dataset configuration (which will be passed to the data loader defined in the cell below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import settings\n",
    "\n",
    "openvino_modelname=settings.SAVED_MODEL_NAME\n",
    "openvino_directory=\"openvino_models\"\n",
    "\n",
    "int8_directory=os.path.join(openvino_directory, \"INT8\")\n",
    "maximum_metric_drop = 0.05  # For accuracy-aware training. this defines how much the metric is allowed to change.\n",
    "openvino_path = os.path.join(openvino_directory, \"FP32\", openvino_modelname)\n",
    "accuracy_aware_quantization=False\n",
    "\n",
    "path_to_xml_file = \"{}.xml\".format(openvino_path)\n",
    "path_to_bin_file = \"{}.bin\".format(openvino_path)\n",
    "\n",
    "dataset_config = {\n",
    "    \"num_samples\": 40,   # Get 40 samples\n",
    "    \"test_dataset\": brats_data.get_test()   # Pass our TensorFlow data loader to the API\n",
    "}\n",
    "\n",
    "model_config = Dict({\n",
    "    \"model_name\": openvino_modelname,\n",
    "    \"model\": path_to_xml_file,\n",
    "    \"weights\": path_to_bin_file\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader for OpenVINO&trade; Post-Training Optimization Toolkit API\n",
    "\n",
    "You can define this data loader to work with your custom dataset. In our case, we've already defined a TensorFlow `tf.data` object. We'll just pass that to the API's data loader and transpose the images and masks (OpenVINO&trade; assumes the data is channels first-- NCHWD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataLoader(DataLoader):\n",
    "\n",
    "    def __init__(self, config):\n",
    "\n",
    "        super().__init__(config)\n",
    "\n",
    "        \"\"\"\n",
    "        You can define this data loader to work with your custom dataset.\n",
    "        In our case, we've already defined a TensorFlow `tf.data` object.\n",
    "        We'll just pass that to the API's data loader and transpose the images and masks\n",
    "        (OpenVINO assumes the data is channels first-- NCHWD)\n",
    "        \"\"\"\n",
    "\n",
    "        self.items = np.arange(config[\"num_samples\"])  # Just pass in how many samples you want to take\n",
    "        self.dataset = config[\"test_dataset\"]\n",
    "\n",
    "        print(bcolors.UNDERLINE + \"\\nQuantizing FP32 OpenVINO model to INT8\\n\" + bcolors.ENDC)\n",
    "\n",
    "        print(bcolors.OKBLUE + \"Taking {:,} random samples from the test dataset \".format(len(self.items)) + \\\n",
    "            bcolors.ENDC)\n",
    "\n",
    "        self.batch_size = 1\n",
    "\n",
    "    def set_subset(self, indices):\n",
    "        self._subset = None\n",
    "\n",
    "    @property\n",
    "    def batch_num(self):\n",
    "        return ceil(self.size / self.batch_size)\n",
    "\n",
    "    @property\n",
    "    def size(self):\n",
    "        return self.items.shape[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        ds = self.dataset.take(1).as_numpy_iterator()  # Grab the next batch and take a single element (image/mask)\n",
    "        for img, msk in ds:\n",
    "            img = np.transpose(img, [0,4,1,2,3])  # OpenVINO expects the input to be channels first (NCHWD)\n",
    "            msk = np.transpose(msk, [0,4,1,2,3])  # OpenVINO expects the label/output to be channels first (NCHWD)\n",
    "        \n",
    "        return (item, msk), img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric for OpenVINO&trade; Post-Training Optimization Toolkit API\n",
    "\n",
    "Here we need to define the metric we will use to evaluate the OpenVINO&trade; model to determine how much it changes when the weights are converted from FP32 to INT8. In our case, the U-Net model uses the Dice coefficient to determine the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMetric(Metric):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.name = \"custom Metric - Dice score\"\n",
    "        self._values = []\n",
    "        self.round = 1\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        \"\"\" Returns accuracy metric value for the last model output. \"\"\"\n",
    "        return {self.name: [self._values[-1]]}\n",
    "\n",
    "    @property\n",
    "    def avg_value(self):\n",
    "        \"\"\" Returns accuracy metric value for all model outputs. \"\"\"\n",
    "        value = np.ravel(self._values).mean()\n",
    "        print(\"Round #{}    Mean {} = {}\".format(self.round, self.name, value))\n",
    "\n",
    "        self.round += 1\n",
    "\n",
    "        return {self.name: value}\n",
    "\n",
    "    def update(self, outputs, labels):\n",
    "        \"\"\" Updates prediction matches.\n",
    "        Args:\n",
    "            outputs: model output\n",
    "            labels: annotations\n",
    "        Put your post-processing code here.\n",
    "        Put your custom metric code here.\n",
    "        The metric gets appended to the list of metric values\n",
    "        \"\"\"\n",
    "\n",
    "        def dice_score(pred, truth):\n",
    "            \"\"\"\n",
    "            Sorensen Dice score\n",
    "            Measure of the overlap between the prediction and ground truth masks\n",
    "            \"\"\"\n",
    "            numerator = np.sum(np.round(pred) * truth) * 2.0\n",
    "            denominator = np.sum(np.round(pred)) + np.sum(truth)\n",
    "\n",
    "            return numerator / denominator\n",
    "\n",
    "\n",
    "        metric = dice_score(labels[0], outputs[0])\n",
    "        self._values.append(metric)\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\" Resets collected matches \"\"\"\n",
    "        self._values = []\n",
    "\n",
    "    @property\n",
    "    def higher_better(self):\n",
    "        \"\"\"Attribute whether the metric should be increased\"\"\"\n",
    "        return True\n",
    "\n",
    "    def get_attributes(self):\n",
    "        return {self.name: {\"direction\": \"higher-better\", \"type\": \"\"}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Calibration Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine_config = Dict({\n",
    "    \"device\": \"CPU\",\n",
    "    \"stat_requests_number\": 4,\n",
    "    \"eval_requests_number\": 4\n",
    "})\n",
    "\n",
    "default_quantization_algorithm = [\n",
    "    {\n",
    "        \"name\": \"DefaultQuantization\",\n",
    "        \"params\": {\n",
    "            \"target_device\": \"CPU\",\n",
    "            \"preset\": \"performance\",\n",
    "            #\"stat_subset_size\": 10\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "accuracy_aware_quantization_algorithm = [\n",
    "    {\n",
    "        \"name\": \"AccuracyAwareQuantization\", # compression algorithm name\n",
    "        \"params\": {\n",
    "            \"target_device\": \"CPU\",\n",
    "            \"preset\": \"performance\",\n",
    "            \"stat_subset_size\": 10,\n",
    "            \"metric_subset_ratio\": 0.5, # A part of the validation set that is used to compare full-precision and quantized models\n",
    "            \"ranking_subset_size\": 300, # A size of a subset which is used to rank layers by their contribution to the accuracy drop\n",
    "            \"max_iter_num\": 10,    # Maximum number of iterations of the algorithm (maximum of layers that may be reverted back to full-precision)\n",
    "            \"maximal_drop\": maximum_metric_drop,      # Maximum metric drop which has to be achieved after the quantization\n",
    "            \"drop_type\": \"absolute\",    # Drop type of the accuracy metric: relative or absolute (default)\n",
    "            \"use_prev_if_drop_increase\": True,     # Whether to use NN snapshot from the previous algorithm iteration in case if drop increases\n",
    "            \"base_algorithm\": \"DefaultQuantization\" # Base algorithm that is used to quantize model at the beginning\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "class GraphAttrs(object):\n",
    "    def __init__(self):\n",
    "        self.keep_quantize_ops_in_IR = True\n",
    "        self.keep_shape_ops = False\n",
    "        self.data_type = \"FP32\"\n",
    "        self.progress = False\n",
    "        self.generate_experimental_IR_V10 = True\n",
    "        self.blobs_as_inputs = True\n",
    "        self.generate_deprecated_IR_V7 = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Post-training optimization Toolkit\n",
    "\n",
    "Once we have the data loader and metric classes defined, we can pass them into the API pipeline. This is calculate the scale (calibration) for the FP32 weights so that they can be converted to INT8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(model_config)\n",
    "\n",
    "data_loader = MyDataLoader(dataset_config)\n",
    "metric = MyMetric()\n",
    "\n",
    "\n",
    "engine = IEEngine(engine_config, data_loader, metric)\n",
    "\n",
    "if accuracy_aware_quantization:\n",
    "    # https://docs.openvinotoolkit.org/latest/_compression_algorithms_quantization_accuracy_aware_README.html\n",
    "    print(bcolors.BOLD + \"Accuracy-aware quantization method\" + bcolors.ENDC)\n",
    "    pipeline = create_pipeline(accuracy_aware_quantization_algorithm, engine)\n",
    "else:\n",
    "    print(bcolors.BOLD + \"Default quantization method\" + bcolors.ENDC)\n",
    "    pipeline = create_pipeline(default_quantization_algorithm, engine)\n",
    "\n",
    "\n",
    "metric_results_FP32 = pipeline.evaluate(model)\n",
    "\n",
    "compressed_model = pipeline.run(model)\n",
    "save_model(compressed_model, int8_directory)\n",
    "\n",
    "metric_results_INT8 = pipeline.evaluate(compressed_model)\n",
    "\n",
    "print(bcolors.BOLD + \"\\nFINAL RESULTS\" + bcolors.ENDC)\n",
    "\n",
    "# print metric value\n",
    "if metric_results_FP32:\n",
    "    for name, value in metric_results_FP32.items():\n",
    "        print(bcolors.OKGREEN + \"{: <27s} FP32: {}\".format(name, value) + bcolors.ENDC)\n",
    "\n",
    "if metric_results_INT8:\n",
    "    for name, value in metric_results_INT8.items():\n",
    "        print(bcolors.OKBLUE + \"{: <27s} INT8: {}\".format(name, value) + bcolors.ENDC)\n",
    "\n",
    "\n",
    "print(bcolors.BOLD + \"\\nThe INT8 version of the model has been saved to the directory \".format(int8_directory) + \\\n",
    "    bcolors.HEADER + \"{}\\n\".format(int8_directory) + bcolors.ENDC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the FP32 OpenVINO&trade; model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_xml_file = \"{}.xml\".format(openvino_path)\n",
    "path_to_bin_file = \"{}.bin\".format(openvino_path)\n",
    "\n",
    "ie = IECore()\n",
    "net = ie.read_network(model=path_to_xml_file, weights=path_to_bin_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the FP32 OpenVINO&trade; model to the hardware device\n",
    "\n",
    "In this case our device is `CPU`. We could also use `MYRIAD` for the Intel&reg; NCS2&trade; VPU or `GPU` for the Intel&reg; GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer_name = next(iter(net.input_info))\n",
    "output_layer_name = next(iter(net.outputs))\n",
    "print(\"Input layer name = {}\\nOutput layer name = {}\".format(input_layer_name, output_layer_name))\n",
    "\n",
    "exec_net = ie.load_network(network=net, device_name=\"CPU\", num_requests=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the INT8 OpenVINO&trade; model to the hardware device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openvino_filename_int8 = os.path.join(int8_directory, openvino_modelname)\n",
    "path_to_xml_file_int8 = \"{}.xml\".format(openvino_filename_int8)\n",
    "path_to_bin_file_int8 = \"{}.bin\".format(openvino_filename_int8)\n",
    "\n",
    "ie_int8 = IECore()\n",
    "net_int8 = ie_int8.read_network(model=path_to_xml_file_int8, weights=path_to_bin_file_int8)\n",
    "\n",
    "input_layer_name = next(iter(net_int8.input_info))\n",
    "output_layer_name = next(iter(net_int8.outputs))\n",
    "print(\"Input layer name = {}\\nOutput layer name = {}\".format(input_layer_name, output_layer_name))\n",
    "\n",
    "exec_net_int8 = ie_int8.load_network(network=net_int8, device_name=\"CPU\", num_requests=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the final TensorFlow model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf_model = tf.keras.models.load_model(\"{}_final\".format(settings.SAVED_MODEL_NAME), compile=False)\n",
    "tf_model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_intel_tensorflow():\n",
    "    \"\"\"\n",
    "    Check if Intel version of TensorFlow is installed\n",
    "    \"\"\"\n",
    "    import tensorflow as tf\n",
    "\n",
    "    print(\"We are using Tensorflow version {}\".format(tf.__version__))\n",
    "\n",
    "    major_version = int(tf.__version__.split(\".\")[0])\n",
    "    if major_version >= 2:\n",
    "        from tensorflow.python import _pywrap_util_port\n",
    "        print(\"Intel-optimizations (DNNL) enabled:\",\n",
    "              _pywrap_util_port.IsMklEnabled())\n",
    "    else:\n",
    "        print(\"Intel-optimizations (DNNL) enabled:\",\n",
    "              tf.pywrap_tensorflow.IsMklEnabled())\n",
    "\n",
    "\n",
    "test_intel_tensorflow()  # Prints if Intel-optimized TensorFlow is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the Dice coefficient\n",
    "\n",
    "This measures the performance of the model from 0 to 1 where 1 means the model gives a perfect prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_dice(target, prediction, smooth=0.0001):\n",
    "    \"\"\"\n",
    "    Sorenson Dice\n",
    "    \"\"\"\n",
    "    prediction = np.round(prediction)\n",
    "\n",
    "    numerator = 2.0 * np.sum(target * prediction) + smooth\n",
    "    denominator = np.sum(target) + np.sum(prediction) + smooth\n",
    "    coef = numerator / denominator\n",
    "\n",
    "    return coef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the predictions for both OpenVINO&trade; and TensorFlow\n",
    "\n",
    "We'll also time the inference to compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(img, msk):\n",
    "    \n",
    "    slicenum=np.argmax(np.sum(msk, axis=(1,2)))  # Find the slice with the largest tumor section\n",
    "\n",
    "    plt.figure(figsize=(20,20))\n",
    "\n",
    "    plt.subplot(1,4,1)\n",
    "    plt.title(\"MRI\", fontsize=20)\n",
    "    plt.imshow(img[0,:,:,slicenum,0], cmap=\"bone\")\n",
    "    plt.subplot(1,4,2)\n",
    "    plt.imshow(msk[0,:,:,slicenum,0], cmap=\"bone\")\n",
    "    plt.title(\"Ground truth\", fontsize=20)\n",
    "\n",
    "    \"\"\"\n",
    "    OpenVINO Model Prediction\n",
    "    Note: OpenVINO assumes the input (and output) are organized as channels first (NCHWD)\n",
    "    whereas TensorFlow assumes channels last (NHWDC). We'll use the NumPy transpose\n",
    "    to change the order.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    res = exec_net.infer({input_layer_name: np.transpose(img, [0,4,1,2,3])})\n",
    "    prediction_ov = np.transpose(res[output_layer_name], [0,2,3,4,1])    \n",
    "    print(\"OpenVINO inference time = {:.4f} msecs\".format(1000.0*(time.time()-start_time)))\n",
    "\n",
    "    plt.subplot(1,4,3)\n",
    "    dice_coef_ov = calc_dice(msk,prediction_ov)\n",
    "    plt.imshow(prediction_ov[0,:,:,slicenum,0], cmap=\"bone\")\n",
    "    plt.title(\"OpenVINO Prediction\\nDice = {:.4f}\".format(dice_coef_ov), fontsize=20)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    TensorFlow Model Prediction\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    prediction_tf = tf_model.predict(img)\n",
    "    print(\"TensorFlow inference time = {:.4f} msecs\".format(1000.0*(time.time()-start_time)))\n",
    "    \n",
    "    plt.subplot(1,4,4)\n",
    "    dice_coef_tf = calc_dice(msk,prediction_tf)\n",
    "    plt.imshow(prediction_tf[0,:,:,slicenum,0], cmap=\"bone\")\n",
    "    plt.title(\"TensorFlow Prediction\\nDice = {:.4f}\".format(dice_coef_tf), fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference time - TensorFlow versus FP32 OpenVINO&trade; \n",
    "\n",
    "Let's grab some data, perform inference, and plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ds = brats_data.get_test().take(1).as_numpy_iterator()\n",
    "for img, msk in ds:\n",
    "    plot_predictions(img,msk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = brats_data.get_test().take(1).as_numpy_iterator()\n",
    "for img, msk in ds:\n",
    "    plot_predictions(img,msk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = brats_data.get_test().take(1).as_numpy_iterator()\n",
    "for img, msk in ds:\n",
    "    plot_predictions(img,msk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = brats_data.get_test().take(1).as_numpy_iterator()\n",
    "for img, msk in ds:\n",
    "    plot_predictions(img,msk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference time - FP32 OpenVINO&trade; versus INT8 OpenVINO&trade;\n",
    "\n",
    "Let's grab some data, perform inference, and plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions_int8_fp32(img, msk):\n",
    "    \n",
    "    slicenum=np.argmax(np.sum(msk, axis=(1,2)))  # Find the slice with the largest tumor section\n",
    "\n",
    "    plt.figure(figsize=(20,20))\n",
    "\n",
    "    plt.subplot(1,4,1)\n",
    "    plt.title(\"MRI\", fontsize=20)\n",
    "    plt.imshow(img[0,:,:,slicenum,0], cmap=\"bone\")\n",
    "    plt.subplot(1,4,2)\n",
    "    plt.imshow(msk[0,:,:,slicenum,0], cmap=\"bone\")\n",
    "    plt.title(\"Ground truth\", fontsize=20)\n",
    "\n",
    "    \"\"\"\n",
    "    OpenVINO Model Prediction - FP32\n",
    "    Note: OpenVINO assumes the input (and output) are organized as channels first (NCHWD)\n",
    "    whereas TensorFlow assumes channels last (NHWDC). We'll use the NumPy transpose\n",
    "    to change the order.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    res = exec_net.infer({input_layer_name: np.transpose(img, [0,4,1,2,3])})\n",
    "    prediction_ov = np.transpose(res[output_layer_name], [0,2,3,4,1])    \n",
    "    print(\"OpenVINO inference time FP32 = {:.4f} msecs\".format(1000.0*(time.time()-start_time)))\n",
    "\n",
    "    plt.subplot(1,4,3)\n",
    "    dice_coef_ov = calc_dice(msk,prediction_ov)\n",
    "    plt.imshow(prediction_ov[0,:,:,slicenum,0], cmap=\"bone\")\n",
    "    plt.title(\"OpenVINO FP32 Prediction\\nDice = {:.4f}\".format(dice_coef_ov), fontsize=20)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    OpenVINO Model Prediction - INT8\n",
    "    Note: OpenVINO assumes the input (and output) are organized as channels first (NCHWD)\n",
    "    whereas TensorFlow assumes channels last (NHWDC). We'll use the NumPy transpose\n",
    "    to change the order.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    res_int8 = exec_net_int8.infer({input_layer_name: np.transpose(img, [0,4,1,2,3])})\n",
    "    prediction_ov_int8 = np.transpose(res_int8[output_layer_name], [0,2,3,4,1])    \n",
    "    print(\"OpenVINO inference time INT8 = {:.4f} msecs\".format(1000.0*(time.time()-start_time)))\n",
    "\n",
    "    plt.subplot(1,4,4)\n",
    "    dice_coef_ov_int8 = calc_dice(msk,prediction_ov_int8)\n",
    "    plt.imshow(prediction_ov_int8[0,:,:,slicenum,0], cmap=\"bone\")\n",
    "    plt.title(\"OpenVINO INT8 Prediction\\nDice = {:.4f}\".format(dice_coef_ov_int8), fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = brats_data.get_test().take(1).as_numpy_iterator()\n",
    "for img, msk in ds:\n",
    "    plot_predictions_int8_fp32(img,msk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = brats_data.get_test().take(1).as_numpy_iterator()\n",
    "for img, msk in ds:\n",
    "    plot_predictions_int8_fp32(img, msk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = brats_data.get_test().take(1).as_numpy_iterator()\n",
    "for img, msk in ds:\n",
    "    plot_predictions_int8_fp32(img, msk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = brats_data.get_test().take(1).as_numpy_iterator()\n",
    "for img, msk in ds:\n",
    "    plot_predictions_int8_fp32(img, msk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = brats_data.get_test().take(1).as_numpy_iterator()\n",
    "for img, msk in ds:\n",
    "    plot_predictions_int8_fp32(img, msk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. SPDX-License-Identifier: EPL-2.0*\n",
    "\n",
    "*Copyright (c) 2019-2020 Intel Corporation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
